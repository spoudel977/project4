{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import warnings\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as py\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV, RandomizedSearchCV\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annual_Income</th>\n",
       "      <th>Credit_History_Age</th>\n",
       "      <th>Credit_Mix</th>\n",
       "      <th>Credit_Utilization_Ratio</th>\n",
       "      <th>Delay_from_due_date</th>\n",
       "      <th>Interest_Rate</th>\n",
       "      <th>Monthly_Inhand_Salary</th>\n",
       "      <th>Num_Bank_Accounts</th>\n",
       "      <th>Num_Credit_Card</th>\n",
       "      <th>Num_Credit_Inquiries</th>\n",
       "      <th>Payment_Behaviour_High_spent_Large_value_payments</th>\n",
       "      <th>Payment_Behaviour_High_spent_Medium_value_payments</th>\n",
       "      <th>Payment_Behaviour_High_spent_Small_value_payments</th>\n",
       "      <th>Payment_Behaviour_Low_spent_Large_value_payments</th>\n",
       "      <th>Payment_Behaviour_Low_spent_Medium_value_payments</th>\n",
       "      <th>Payment_Behaviour_Low_spent_Small_value_payments</th>\n",
       "      <th>Total_EMI_per_month</th>\n",
       "      <th>Credit_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.380808</td>\n",
       "      <td>0.899901</td>\n",
       "      <td>-0.226664</td>\n",
       "      <td>-1.533269</td>\n",
       "      <td>-1.193772</td>\n",
       "      <td>-0.144561</td>\n",
       "      <td>-0.334169</td>\n",
       "      <td>-0.124313</td>\n",
       "      <td>-0.145076</td>\n",
       "      <td>-0.130134</td>\n",
       "      <td>-0.400697</td>\n",
       "      <td>-0.465473</td>\n",
       "      <td>-0.362397</td>\n",
       "      <td>-0.343194</td>\n",
       "      <td>-0.396962</td>\n",
       "      <td>1.723317</td>\n",
       "      <td>-0.839388</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.380808</td>\n",
       "      <td>0.920317</td>\n",
       "      <td>-0.226664</td>\n",
       "      <td>0.184213</td>\n",
       "      <td>-1.193772</td>\n",
       "      <td>-0.144561</td>\n",
       "      <td>-0.334169</td>\n",
       "      <td>-0.124313</td>\n",
       "      <td>10.382238</td>\n",
       "      <td>-0.130134</td>\n",
       "      <td>2.495649</td>\n",
       "      <td>-0.465473</td>\n",
       "      <td>-0.362397</td>\n",
       "      <td>-0.343194</td>\n",
       "      <td>-0.396962</td>\n",
       "      <td>-0.580276</td>\n",
       "      <td>-0.839388</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.687132</td>\n",
       "      <td>-0.120924</td>\n",
       "      <td>-0.226664</td>\n",
       "      <td>1.133762</td>\n",
       "      <td>-0.810326</td>\n",
       "      <td>-0.140453</td>\n",
       "      <td>2.768815</td>\n",
       "      <td>-0.133005</td>\n",
       "      <td>-0.137453</td>\n",
       "      <td>-0.124893</td>\n",
       "      <td>-0.400697</td>\n",
       "      <td>-0.465473</td>\n",
       "      <td>2.759407</td>\n",
       "      <td>-0.343194</td>\n",
       "      <td>-0.396962</td>\n",
       "      <td>-0.580276</td>\n",
       "      <td>1.994874</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.687132</td>\n",
       "      <td>-0.110716</td>\n",
       "      <td>-0.226664</td>\n",
       "      <td>1.193775</td>\n",
       "      <td>-0.810326</td>\n",
       "      <td>-0.140453</td>\n",
       "      <td>2.768815</td>\n",
       "      <td>-0.133005</td>\n",
       "      <td>-0.137453</td>\n",
       "      <td>-0.124893</td>\n",
       "      <td>-0.400697</td>\n",
       "      <td>-0.465473</td>\n",
       "      <td>2.759407</td>\n",
       "      <td>-0.343194</td>\n",
       "      <td>-0.396962</td>\n",
       "      <td>-0.580276</td>\n",
       "      <td>1.994874</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.498578</td>\n",
       "      <td>-0.243423</td>\n",
       "      <td>-0.226664</td>\n",
       "      <td>-1.125469</td>\n",
       "      <td>-1.423840</td>\n",
       "      <td>-0.148669</td>\n",
       "      <td>-0.478477</td>\n",
       "      <td>-0.124313</td>\n",
       "      <td>-0.137453</td>\n",
       "      <td>-0.119651</td>\n",
       "      <td>-0.400697</td>\n",
       "      <td>-0.465473</td>\n",
       "      <td>-0.362397</td>\n",
       "      <td>2.913802</td>\n",
       "      <td>-0.396962</td>\n",
       "      <td>-0.580276</td>\n",
       "      <td>-0.869208</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111809</th>\n",
       "      <td>-0.499641</td>\n",
       "      <td>1.410314</td>\n",
       "      <td>-0.226664</td>\n",
       "      <td>-0.652615</td>\n",
       "      <td>-0.426880</td>\n",
       "      <td>-0.142507</td>\n",
       "      <td>-0.402550</td>\n",
       "      <td>-0.115620</td>\n",
       "      <td>-0.122207</td>\n",
       "      <td>-0.119651</td>\n",
       "      <td>-0.400697</td>\n",
       "      <td>-0.465473</td>\n",
       "      <td>-0.362397</td>\n",
       "      <td>-0.343194</td>\n",
       "      <td>-0.396962</td>\n",
       "      <td>1.723317</td>\n",
       "      <td>-0.388896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111810</th>\n",
       "      <td>-0.881550</td>\n",
       "      <td>-0.029050</td>\n",
       "      <td>-0.226664</td>\n",
       "      <td>-0.348059</td>\n",
       "      <td>-0.580258</td>\n",
       "      <td>-0.148669</td>\n",
       "      <td>-0.876556</td>\n",
       "      <td>-0.106927</td>\n",
       "      <td>-0.145076</td>\n",
       "      <td>-0.119651</td>\n",
       "      <td>-0.400697</td>\n",
       "      <td>-0.465473</td>\n",
       "      <td>-0.362397</td>\n",
       "      <td>2.913802</td>\n",
       "      <td>-0.396962</td>\n",
       "      <td>-0.580276</td>\n",
       "      <td>-0.869778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111811</th>\n",
       "      <td>-0.355200</td>\n",
       "      <td>1.338856</td>\n",
       "      <td>-0.226664</td>\n",
       "      <td>-1.005398</td>\n",
       "      <td>-1.347151</td>\n",
       "      <td>-0.136345</td>\n",
       "      <td>-0.301834</td>\n",
       "      <td>-0.098234</td>\n",
       "      <td>-0.137453</td>\n",
       "      <td>-0.140618</td>\n",
       "      <td>-0.400697</td>\n",
       "      <td>2.148350</td>\n",
       "      <td>-0.362397</td>\n",
       "      <td>-0.343194</td>\n",
       "      <td>-0.396962</td>\n",
       "      <td>-0.580276</td>\n",
       "      <td>-1.073111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111812</th>\n",
       "      <td>2.040500</td>\n",
       "      <td>0.032199</td>\n",
       "      <td>-0.226664</td>\n",
       "      <td>-0.722303</td>\n",
       "      <td>-0.810326</td>\n",
       "      <td>-0.150723</td>\n",
       "      <td>2.110701</td>\n",
       "      <td>-0.072156</td>\n",
       "      <td>-0.145076</td>\n",
       "      <td>-0.140618</td>\n",
       "      <td>-0.400697</td>\n",
       "      <td>2.148350</td>\n",
       "      <td>-0.362397</td>\n",
       "      <td>-0.343194</td>\n",
       "      <td>-0.396962</td>\n",
       "      <td>-0.580276</td>\n",
       "      <td>-0.230705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111813</th>\n",
       "      <td>0.579497</td>\n",
       "      <td>1.308231</td>\n",
       "      <td>-0.226664</td>\n",
       "      <td>-1.253567</td>\n",
       "      <td>0.800149</td>\n",
       "      <td>-0.136345</td>\n",
       "      <td>0.490082</td>\n",
       "      <td>-0.089542</td>\n",
       "      <td>-0.152699</td>\n",
       "      <td>-0.109168</td>\n",
       "      <td>2.495649</td>\n",
       "      <td>-0.465473</td>\n",
       "      <td>-0.362397</td>\n",
       "      <td>-0.343194</td>\n",
       "      <td>-0.396962</td>\n",
       "      <td>-0.580276</td>\n",
       "      <td>-1.073111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111814 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Annual_Income  Credit_History_Age  Credit_Mix  \\\n",
       "0           -0.380808            0.899901   -0.226664   \n",
       "1           -0.380808            0.920317   -0.226664   \n",
       "2            2.687132           -0.120924   -0.226664   \n",
       "3            2.687132           -0.110716   -0.226664   \n",
       "4           -0.498578           -0.243423   -0.226664   \n",
       "...               ...                 ...         ...   \n",
       "111809      -0.499641            1.410314   -0.226664   \n",
       "111810      -0.881550           -0.029050   -0.226664   \n",
       "111811      -0.355200            1.338856   -0.226664   \n",
       "111812       2.040500            0.032199   -0.226664   \n",
       "111813       0.579497            1.308231   -0.226664   \n",
       "\n",
       "        Credit_Utilization_Ratio  Delay_from_due_date  Interest_Rate  \\\n",
       "0                      -1.533269            -1.193772      -0.144561   \n",
       "1                       0.184213            -1.193772      -0.144561   \n",
       "2                       1.133762            -0.810326      -0.140453   \n",
       "3                       1.193775            -0.810326      -0.140453   \n",
       "4                      -1.125469            -1.423840      -0.148669   \n",
       "...                          ...                  ...            ...   \n",
       "111809                 -0.652615            -0.426880      -0.142507   \n",
       "111810                 -0.348059            -0.580258      -0.148669   \n",
       "111811                 -1.005398            -1.347151      -0.136345   \n",
       "111812                 -0.722303            -0.810326      -0.150723   \n",
       "111813                 -1.253567             0.800149      -0.136345   \n",
       "\n",
       "        Monthly_Inhand_Salary  Num_Bank_Accounts  Num_Credit_Card  \\\n",
       "0                   -0.334169          -0.124313        -0.145076   \n",
       "1                   -0.334169          -0.124313        10.382238   \n",
       "2                    2.768815          -0.133005        -0.137453   \n",
       "3                    2.768815          -0.133005        -0.137453   \n",
       "4                   -0.478477          -0.124313        -0.137453   \n",
       "...                       ...                ...              ...   \n",
       "111809              -0.402550          -0.115620        -0.122207   \n",
       "111810              -0.876556          -0.106927        -0.145076   \n",
       "111811              -0.301834          -0.098234        -0.137453   \n",
       "111812               2.110701          -0.072156        -0.145076   \n",
       "111813               0.490082          -0.089542        -0.152699   \n",
       "\n",
       "        Num_Credit_Inquiries  \\\n",
       "0                  -0.130134   \n",
       "1                  -0.130134   \n",
       "2                  -0.124893   \n",
       "3                  -0.124893   \n",
       "4                  -0.119651   \n",
       "...                      ...   \n",
       "111809             -0.119651   \n",
       "111810             -0.119651   \n",
       "111811             -0.140618   \n",
       "111812             -0.140618   \n",
       "111813             -0.109168   \n",
       "\n",
       "        Payment_Behaviour_High_spent_Large_value_payments  \\\n",
       "0                                               -0.400697   \n",
       "1                                                2.495649   \n",
       "2                                               -0.400697   \n",
       "3                                               -0.400697   \n",
       "4                                               -0.400697   \n",
       "...                                                   ...   \n",
       "111809                                          -0.400697   \n",
       "111810                                          -0.400697   \n",
       "111811                                          -0.400697   \n",
       "111812                                          -0.400697   \n",
       "111813                                           2.495649   \n",
       "\n",
       "        Payment_Behaviour_High_spent_Medium_value_payments  \\\n",
       "0                                               -0.465473    \n",
       "1                                               -0.465473    \n",
       "2                                               -0.465473    \n",
       "3                                               -0.465473    \n",
       "4                                               -0.465473    \n",
       "...                                                   ...    \n",
       "111809                                          -0.465473    \n",
       "111810                                          -0.465473    \n",
       "111811                                           2.148350    \n",
       "111812                                           2.148350    \n",
       "111813                                          -0.465473    \n",
       "\n",
       "        Payment_Behaviour_High_spent_Small_value_payments  \\\n",
       "0                                               -0.362397   \n",
       "1                                               -0.362397   \n",
       "2                                                2.759407   \n",
       "3                                                2.759407   \n",
       "4                                               -0.362397   \n",
       "...                                                   ...   \n",
       "111809                                          -0.362397   \n",
       "111810                                          -0.362397   \n",
       "111811                                          -0.362397   \n",
       "111812                                          -0.362397   \n",
       "111813                                          -0.362397   \n",
       "\n",
       "        Payment_Behaviour_Low_spent_Large_value_payments  \\\n",
       "0                                              -0.343194   \n",
       "1                                              -0.343194   \n",
       "2                                              -0.343194   \n",
       "3                                              -0.343194   \n",
       "4                                               2.913802   \n",
       "...                                                  ...   \n",
       "111809                                         -0.343194   \n",
       "111810                                          2.913802   \n",
       "111811                                         -0.343194   \n",
       "111812                                         -0.343194   \n",
       "111813                                         -0.343194   \n",
       "\n",
       "        Payment_Behaviour_Low_spent_Medium_value_payments  \\\n",
       "0                                               -0.396962   \n",
       "1                                               -0.396962   \n",
       "2                                               -0.396962   \n",
       "3                                               -0.396962   \n",
       "4                                               -0.396962   \n",
       "...                                                   ...   \n",
       "111809                                          -0.396962   \n",
       "111810                                          -0.396962   \n",
       "111811                                          -0.396962   \n",
       "111812                                          -0.396962   \n",
       "111813                                          -0.396962   \n",
       "\n",
       "        Payment_Behaviour_Low_spent_Small_value_payments  Total_EMI_per_month  \\\n",
       "0                                               1.723317            -0.839388   \n",
       "1                                              -0.580276            -0.839388   \n",
       "2                                              -0.580276             1.994874   \n",
       "3                                              -0.580276             1.994874   \n",
       "4                                              -0.580276            -0.869208   \n",
       "...                                                  ...                  ...   \n",
       "111809                                          1.723317            -0.388896   \n",
       "111810                                         -0.580276            -0.869778   \n",
       "111811                                         -0.580276            -1.073111   \n",
       "111812                                         -0.580276            -0.230705   \n",
       "111813                                         -0.580276            -1.073111   \n",
       "\n",
       "        Credit_Score  \n",
       "0                  2  \n",
       "1                  2  \n",
       "2                  2  \n",
       "3                  2  \n",
       "4                  2  \n",
       "...              ...  \n",
       "111809             0  \n",
       "111810             0  \n",
       "111811             0  \n",
       "111812             0  \n",
       "111813             0  \n",
       "\n",
       "[111814 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df_final_without_outliers  = pd.read_csv(\"/Users/smritilimbu/Desktop/Machine_Learning_Project4/df_finalclean.csv\")\n",
    "df_final_without_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Clean Data into Test and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (89451, 17)\n",
      "y_train.shape =  (89451,)\n",
      "x_val.shape =  (22363, 17)\n",
      "y_val.shape =  (22363,)\n"
     ]
    }
   ],
   "source": [
    "# features = credit_score.drop(columns = ['Credit_Score','Age','Num_Bank_Accounts','Num_Credit_Card','Interest_Rate','Num_of_Loan','Delay_from_due_date','Num_Credit_Inquiries','Num_of_Delayed_Payment','Payment_of_Min_Amount'])\n",
    "# label = credit_score['Credit_Score']\n",
    "\n",
    "X = df_final_without_outliers.drop(columns='Credit_Score')\n",
    "y = df_final_without_outliers['Credit_Score']\n",
    "\n",
    "# print(\"features = \",features)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42,\n",
    "                                                  shuffle=True)\n",
    "print(\"x_train.shape = \", X_train.shape)\n",
    "print(\"y_train.shape = \", y_train.shape)\n",
    "print(\"x_val.shape = \", X_val.shape)\n",
    "print(\"y_val.shape = \", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unoptimized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Logistic Regression] - Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.81      0.74      8056\n",
      "           1       0.70      0.64      0.67      7946\n",
      "           2       0.54      0.45      0.49      6361\n",
      "\n",
      "    accuracy                           0.65     22363\n",
      "   macro avg       0.64      0.63      0.63     22363\n",
      "weighted avg       0.64      0.65      0.64     22363\n",
      "\n",
      "Accuracy: 0.6484818673702097\n",
      "\n",
      "[Decision Tree] - Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.85      0.80      8056\n",
      "           1       0.80      0.79      0.79      7946\n",
      "           2       0.68      0.57      0.62      6361\n",
      "\n",
      "    accuracy                           0.75     22363\n",
      "   macro avg       0.74      0.74      0.74     22363\n",
      "weighted avg       0.75      0.75      0.75     22363\n",
      "\n",
      "Accuracy: 0.7498099539417789\n",
      "\n",
      "[Random Forest] - Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.84      0.79      8056\n",
      "           1       0.80      0.77      0.79      7946\n",
      "           2       0.67      0.59      0.63      6361\n",
      "\n",
      "    accuracy                           0.75     22363\n",
      "   macro avg       0.74      0.74      0.74     22363\n",
      "weighted avg       0.74      0.75      0.74     22363\n",
      "\n",
      "Accuracy: 0.7464114832535885\n",
      "\n",
      "[Gradient Boosting] - Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.83      0.78      8056\n",
      "           1       0.79      0.74      0.76      7946\n",
      "           2       0.65      0.58      0.61      6361\n",
      "\n",
      "    accuracy                           0.73     22363\n",
      "   macro avg       0.72      0.72      0.72     22363\n",
      "weighted avg       0.73      0.73      0.73     22363\n",
      "\n",
      "Accuracy: 0.7283012118231007\n",
      "\n",
      "[Support Vector Machine] - Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.82      0.76      8056\n",
      "           1       0.71      0.70      0.70      7946\n",
      "           2       0.59      0.49      0.54      6361\n",
      "\n",
      "    accuracy                           0.68     22363\n",
      "   macro avg       0.67      0.67      0.67     22363\n",
      "weighted avg       0.68      0.68      0.68     22363\n",
      "\n",
      "Accuracy: 0.68237714081295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# ML models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, solver='lbfgs', C=1.0, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
    "    'Support Vector Machine': SVC(probability=True, C=1.0, kernel='rbf', gamma='scale', random_state=42)\n",
    "}\n",
    "\n",
    "# train the models and evaluate the metrics\n",
    "result = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # train the ml model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict using the test set\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    result[name] = accuracy\n",
    "\n",
    "    # classification report\n",
    "    print(f\"[{name}] - Classification Report: \")\n",
    "    report = classification_report(y_val, y_pred)\n",
    "    print(report)\n",
    "    print(f\"Accuracy: {accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of all model accuracies: \n",
      "Logistic Regression: 64.85%\n",
      "Decision Tree: 74.98%\n",
      "Random Forest: 74.64%\n",
      "Gradient Boosting: 72.83%\n",
      "Support Vector Machine: 68.24%\n"
     ]
    }
   ],
   "source": [
    "# Acduracy summaries of all models \n",
    "print(\"Summary of all model accuracies: \")\n",
    "for name, accuracy in result.items():\n",
    "    print(f\"{name}: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Optimization for all Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Logistic Regression...\n",
      "[Logistic Regression] - Best Parameters: {'solver': 'lbfgs', 'max_iter': 500, 'C': 1.0}\n",
      "[Logistic Regression] - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.81      0.74      8056\n",
      "           1       0.70      0.64      0.67      7946\n",
      "           2       0.54      0.45      0.49      6361\n",
      "\n",
      "    accuracy                           0.65     22363\n",
      "   macro avg       0.64      0.63      0.63     22363\n",
      "weighted avg       0.64      0.65      0.64     22363\n",
      "\n",
      "Accuracy: 0.6484818673702097\n",
      "\n",
      "Optimizing Decision Tree...\n",
      "[Decision Tree] - Best Parameters: {'min_samples_split': 2, 'min_samples_leaf': 5, 'max_depth': 20}\n",
      "[Decision Tree] - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88      8056\n",
      "           1       0.85      0.89      0.87      7946\n",
      "           2       0.79      0.62      0.69      6361\n",
      "\n",
      "    accuracy                           0.83     22363\n",
      "   macro avg       0.82      0.81      0.81     22363\n",
      "weighted avg       0.82      0.83      0.82     22363\n",
      "\n",
      "Accuracy: 0.826186110986898\n",
      "\n",
      "Optimizing Random Forest...\n",
      "[Random Forest] - Best Parameters: {'n_estimators': 200, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 20}\n",
      "[Random Forest] - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89      8056\n",
      "           1       0.85      0.89      0.87      7946\n",
      "           2       0.81      0.63      0.71      6361\n",
      "\n",
      "    accuracy                           0.84     22363\n",
      "   macro avg       0.83      0.82      0.82     22363\n",
      "weighted avg       0.83      0.84      0.83     22363\n",
      "\n",
      "Accuracy: 0.8359790725752358\n",
      "\n",
      "Optimizing Gradient Boosting...\n",
      "[Gradient Boosting] - Best Parameters: {'n_estimators': 200, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 7, 'learning_rate': 0.2}\n",
      "[Gradient Boosting] - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92      8056\n",
      "           1       0.88      0.93      0.90      7946\n",
      "           2       0.86      0.70      0.77      6361\n",
      "\n",
      "    accuracy                           0.88     22363\n",
      "   macro avg       0.87      0.86      0.87     22363\n",
      "weighted avg       0.88      0.88      0.87     22363\n",
      "\n",
      "Accuracy: 0.8761794034789608\n",
      "\n",
      "Optimizing Support Vector Machine...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'solver': ['lbfgs', 'liblinear'],\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'max_iter': [100, 500, 1000]\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10, 20],\n",
    "        'min_samples_split': [2, 10, 20],\n",
    "        'min_samples_leaf': [1, 5, 10]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [5, 10, 20],\n",
    "        'min_samples_split': [2, 10, 20],\n",
    "        'min_samples_leaf': [1, 5, 10]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 10, 20],\n",
    "        'min_samples_leaf': [1, 5, 10]\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "# ML models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Results storage\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Perform RandomizedSearchCV\n",
    "    print(f\"Optimizing {name}...\")\n",
    "    param_grid = param_grids[name]\n",
    "    randomized_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "    # Fit model\n",
    "    randomized_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model\n",
    "    best_model = randomized_search.best_estimator_\n",
    "\n",
    "    # Predict using the test set\n",
    "    y_pred = best_model.predict(X_val)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    results[name] = accuracy\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"[{name}] - Best Parameters: {randomized_search.best_params_}\")\n",
    "    print(f\"[{name}] - Classification Report:\")\n",
    "    report = classification_report(y_val, y_pred)\n",
    "    print(report)\n",
    "    print(f\"Accuracy: {accuracy}\\n\")\n",
    "\n",
    "# Display summarized results\n",
    "print(\"Summary of Results:\")\n",
    "for name, accuracy in results.items():\n",
    "    print(f\"{name}: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using neural networks for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smritilimbu/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 438us/step - accuracy: 0.5523 - loss: 0.9505 - val_accuracy: 0.6729 - val_loss: 0.8165\n",
      "Epoch 2/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - accuracy: 0.6630 - loss: 0.8266 - val_accuracy: 0.6785 - val_loss: 0.8034\n",
      "Epoch 3/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389us/step - accuracy: 0.6748 - loss: 0.8019 - val_accuracy: 0.6814 - val_loss: 0.7856\n",
      "Epoch 4/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - accuracy: 0.6770 - loss: 0.7975 - val_accuracy: 0.6810 - val_loss: 0.7819\n",
      "Epoch 5/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - accuracy: 0.6758 - loss: 0.7939 - val_accuracy: 0.6828 - val_loss: 0.7800\n",
      "Epoch 6/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388us/step - accuracy: 0.6778 - loss: 0.7888 - val_accuracy: 0.6825 - val_loss: 0.7769\n",
      "Epoch 7/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - accuracy: 0.6792 - loss: 0.7858 - val_accuracy: 0.6833 - val_loss: 0.7812\n",
      "Epoch 8/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 387us/step - accuracy: 0.6784 - loss: 0.7857 - val_accuracy: 0.6854 - val_loss: 0.7745\n",
      "Epoch 9/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - accuracy: 0.6809 - loss: 0.7825 - val_accuracy: 0.6846 - val_loss: 0.7708\n",
      "Epoch 10/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - accuracy: 0.6813 - loss: 0.7821 - val_accuracy: 0.6859 - val_loss: 0.7798\n",
      "Epoch 11/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 386us/step - accuracy: 0.6804 - loss: 0.7834 - val_accuracy: 0.6844 - val_loss: 0.7780\n",
      "Epoch 12/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 387us/step - accuracy: 0.6805 - loss: 0.7828 - val_accuracy: 0.6858 - val_loss: 0.7767\n",
      "Epoch 13/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - accuracy: 0.6799 - loss: 0.7816 - val_accuracy: 0.6849 - val_loss: 0.7711\n",
      "Epoch 14/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - accuracy: 0.6807 - loss: 0.7774 - val_accuracy: 0.6868 - val_loss: 0.7681\n",
      "Epoch 15/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 406us/step - accuracy: 0.6852 - loss: 0.7736 - val_accuracy: 0.6866 - val_loss: 0.7696\n",
      "Epoch 16/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - accuracy: 0.6820 - loss: 0.7798 - val_accuracy: 0.6852 - val_loss: 0.7715\n",
      "Epoch 17/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389us/step - accuracy: 0.6844 - loss: 0.7757 - val_accuracy: 0.6860 - val_loss: 0.7682\n",
      "Epoch 18/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - accuracy: 0.6847 - loss: 0.7737 - val_accuracy: 0.6859 - val_loss: 0.7694\n",
      "Epoch 19/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - accuracy: 0.6861 - loss: 0.7721 - val_accuracy: 0.6858 - val_loss: 0.7694\n",
      "Epoch 20/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 390us/step - accuracy: 0.6865 - loss: 0.7736 - val_accuracy: 0.6876 - val_loss: 0.7690\n",
      "Epoch 21/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - accuracy: 0.6879 - loss: 0.7701 - val_accuracy: 0.6880 - val_loss: 0.7684\n",
      "Epoch 22/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 390us/step - accuracy: 0.6868 - loss: 0.7723 - val_accuracy: 0.6885 - val_loss: 0.7697\n",
      "Epoch 23/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - accuracy: 0.6856 - loss: 0.7751 - val_accuracy: 0.6901 - val_loss: 0.7685\n",
      "Epoch 24/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - accuracy: 0.6864 - loss: 0.7723 - val_accuracy: 0.6909 - val_loss: 0.7657\n",
      "Epoch 25/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - accuracy: 0.6871 - loss: 0.7721 - val_accuracy: 0.6945 - val_loss: 0.7642\n",
      "Epoch 26/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - accuracy: 0.6890 - loss: 0.7685 - val_accuracy: 0.6954 - val_loss: 0.7577\n",
      "Epoch 27/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - accuracy: 0.6910 - loss: 0.7684 - val_accuracy: 0.6927 - val_loss: 0.7745\n",
      "Epoch 28/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - accuracy: 0.6943 - loss: 0.7658 - val_accuracy: 0.6995 - val_loss: 0.7583\n",
      "Epoch 29/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - accuracy: 0.6940 - loss: 0.7629 - val_accuracy: 0.6992 - val_loss: 0.7528\n",
      "Epoch 30/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 390us/step - accuracy: 0.6921 - loss: 0.7656 - val_accuracy: 0.7025 - val_loss: 0.7517\n",
      "Epoch 31/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396us/step - accuracy: 0.6949 - loss: 0.7641 - val_accuracy: 0.6987 - val_loss: 0.7587\n",
      "Epoch 32/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 402us/step - accuracy: 0.6941 - loss: 0.7655 - val_accuracy: 0.7031 - val_loss: 0.7522\n",
      "Epoch 33/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - accuracy: 0.7004 - loss: 0.7578 - val_accuracy: 0.7067 - val_loss: 0.7426\n",
      "Epoch 34/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - accuracy: 0.6971 - loss: 0.7628 - val_accuracy: 0.7015 - val_loss: 0.7551\n",
      "Epoch 35/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - accuracy: 0.6995 - loss: 0.7569 - val_accuracy: 0.7060 - val_loss: 0.7455\n",
      "Epoch 36/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - accuracy: 0.7024 - loss: 0.7564 - val_accuracy: 0.7062 - val_loss: 0.7523\n",
      "Epoch 37/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - accuracy: 0.7007 - loss: 0.7574 - val_accuracy: 0.7051 - val_loss: 0.7449\n",
      "Epoch 38/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - accuracy: 0.7011 - loss: 0.7584 - val_accuracy: 0.7092 - val_loss: 0.7421\n",
      "Epoch 39/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - accuracy: 0.7011 - loss: 0.7574 - val_accuracy: 0.7100 - val_loss: 0.7432\n",
      "Epoch 40/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396us/step - accuracy: 0.7004 - loss: 0.7579 - val_accuracy: 0.7096 - val_loss: 0.7409\n",
      "Epoch 41/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - accuracy: 0.7017 - loss: 0.7539 - val_accuracy: 0.7096 - val_loss: 0.7443\n",
      "Epoch 42/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - accuracy: 0.7023 - loss: 0.7548 - val_accuracy: 0.7086 - val_loss: 0.7398\n",
      "Epoch 43/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - accuracy: 0.7043 - loss: 0.7529 - val_accuracy: 0.7059 - val_loss: 0.7511\n",
      "Epoch 44/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 413us/step - accuracy: 0.7012 - loss: 0.7523 - val_accuracy: 0.7068 - val_loss: 0.7493\n",
      "Epoch 45/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 402us/step - accuracy: 0.7018 - loss: 0.7548 - val_accuracy: 0.7101 - val_loss: 0.7451\n",
      "Epoch 46/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401us/step - accuracy: 0.7041 - loss: 0.7520 - val_accuracy: 0.7076 - val_loss: 0.7466\n",
      "Epoch 47/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403us/step - accuracy: 0.7034 - loss: 0.7550 - val_accuracy: 0.7104 - val_loss: 0.7363\n",
      "Epoch 48/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401us/step - accuracy: 0.7066 - loss: 0.7500 - val_accuracy: 0.7085 - val_loss: 0.7412\n",
      "Epoch 49/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - accuracy: 0.7014 - loss: 0.7549 - val_accuracy: 0.7103 - val_loss: 0.7413\n",
      "Epoch 50/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 405us/step - accuracy: 0.7055 - loss: 0.7501 - val_accuracy: 0.7095 - val_loss: 0.7410\n",
      "Epoch 51/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - accuracy: 0.7023 - loss: 0.7553 - val_accuracy: 0.7088 - val_loss: 0.7401\n",
      "Epoch 52/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - accuracy: 0.7012 - loss: 0.7524 - val_accuracy: 0.7079 - val_loss: 0.7420\n",
      "Epoch 53/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 399us/step - accuracy: 0.7036 - loss: 0.7527 - val_accuracy: 0.7056 - val_loss: 0.7441\n",
      "Epoch 54/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - accuracy: 0.7044 - loss: 0.7503 - val_accuracy: 0.7058 - val_loss: 0.7417\n",
      "Epoch 55/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401us/step - accuracy: 0.7053 - loss: 0.7482 - val_accuracy: 0.7092 - val_loss: 0.7380\n",
      "Epoch 56/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - accuracy: 0.7051 - loss: 0.7502 - val_accuracy: 0.7088 - val_loss: 0.7382\n",
      "Epoch 57/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401us/step - accuracy: 0.7014 - loss: 0.7532 - val_accuracy: 0.7088 - val_loss: 0.7398\n",
      "Epoch 58/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396us/step - accuracy: 0.7053 - loss: 0.7478 - val_accuracy: 0.7072 - val_loss: 0.7423\n",
      "Epoch 59/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389us/step - accuracy: 0.7022 - loss: 0.7500 - val_accuracy: 0.7075 - val_loss: 0.7413\n",
      "Epoch 60/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385us/step - accuracy: 0.7040 - loss: 0.7476 - val_accuracy: 0.7079 - val_loss: 0.7391\n",
      "Epoch 61/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step - accuracy: 0.7068 - loss: 0.7489 - val_accuracy: 0.7118 - val_loss: 0.7352\n",
      "Epoch 62/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 390us/step - accuracy: 0.7047 - loss: 0.7496 - val_accuracy: 0.7081 - val_loss: 0.7429\n",
      "Epoch 63/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - accuracy: 0.7032 - loss: 0.7521 - val_accuracy: 0.7099 - val_loss: 0.7396\n",
      "Epoch 64/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 399us/step - accuracy: 0.7033 - loss: 0.7507 - val_accuracy: 0.7080 - val_loss: 0.7355\n",
      "Epoch 65/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - accuracy: 0.7041 - loss: 0.7486 - val_accuracy: 0.7080 - val_loss: 0.7416\n",
      "Epoch 66/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - accuracy: 0.7064 - loss: 0.7495 - val_accuracy: 0.7089 - val_loss: 0.7341\n",
      "Epoch 67/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - accuracy: 0.7030 - loss: 0.7513 - val_accuracy: 0.7100 - val_loss: 0.7359\n",
      "Epoch 68/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 399us/step - accuracy: 0.7055 - loss: 0.7483 - val_accuracy: 0.7093 - val_loss: 0.7395\n",
      "Epoch 69/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - accuracy: 0.7044 - loss: 0.7489 - val_accuracy: 0.7083 - val_loss: 0.7372\n",
      "Epoch 70/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - accuracy: 0.7070 - loss: 0.7453 - val_accuracy: 0.7058 - val_loss: 0.7444\n",
      "Epoch 71/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - accuracy: 0.7030 - loss: 0.7525 - val_accuracy: 0.7094 - val_loss: 0.7367\n",
      "Epoch 72/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - accuracy: 0.7067 - loss: 0.7478 - val_accuracy: 0.7080 - val_loss: 0.7364\n",
      "Epoch 73/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - accuracy: 0.7075 - loss: 0.7445 - val_accuracy: 0.7078 - val_loss: 0.7371\n",
      "Epoch 74/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396us/step - accuracy: 0.7067 - loss: 0.7450 - val_accuracy: 0.7076 - val_loss: 0.7360\n",
      "Epoch 75/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - accuracy: 0.7080 - loss: 0.7442 - val_accuracy: 0.7086 - val_loss: 0.7354\n",
      "Epoch 76/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - accuracy: 0.7033 - loss: 0.7508 - val_accuracy: 0.7088 - val_loss: 0.7407\n",
      "Epoch 77/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401us/step - accuracy: 0.7066 - loss: 0.7489 - val_accuracy: 0.7084 - val_loss: 0.7421\n",
      "Epoch 78/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - accuracy: 0.7051 - loss: 0.7490 - val_accuracy: 0.7133 - val_loss: 0.7384\n",
      "Epoch 79/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 399us/step - accuracy: 0.7082 - loss: 0.7451 - val_accuracy: 0.7119 - val_loss: 0.7424\n",
      "Epoch 80/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - accuracy: 0.7055 - loss: 0.7491 - val_accuracy: 0.7114 - val_loss: 0.7333\n",
      "Epoch 81/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - accuracy: 0.7037 - loss: 0.7488 - val_accuracy: 0.7090 - val_loss: 0.7360\n",
      "Epoch 82/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - accuracy: 0.7064 - loss: 0.7460 - val_accuracy: 0.7100 - val_loss: 0.7318\n",
      "Epoch 83/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - accuracy: 0.7064 - loss: 0.7460 - val_accuracy: 0.7064 - val_loss: 0.7413\n",
      "Epoch 84/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - accuracy: 0.7063 - loss: 0.7468 - val_accuracy: 0.7080 - val_loss: 0.7418\n",
      "Epoch 85/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 399us/step - accuracy: 0.7087 - loss: 0.7442 - val_accuracy: 0.7099 - val_loss: 0.7353\n",
      "Epoch 86/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - accuracy: 0.7047 - loss: 0.7484 - val_accuracy: 0.7111 - val_loss: 0.7309\n",
      "Epoch 87/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - accuracy: 0.7069 - loss: 0.7472 - val_accuracy: 0.7079 - val_loss: 0.7324\n",
      "Epoch 88/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - accuracy: 0.7072 - loss: 0.7466 - val_accuracy: 0.7097 - val_loss: 0.7359\n",
      "Epoch 89/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - accuracy: 0.7089 - loss: 0.7428 - val_accuracy: 0.7099 - val_loss: 0.7312\n",
      "Epoch 90/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - accuracy: 0.7074 - loss: 0.7463 - val_accuracy: 0.7083 - val_loss: 0.7322\n",
      "Epoch 91/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401us/step - accuracy: 0.7061 - loss: 0.7441 - val_accuracy: 0.7090 - val_loss: 0.7339\n",
      "Epoch 92/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - accuracy: 0.7049 - loss: 0.7479 - val_accuracy: 0.7101 - val_loss: 0.7342\n",
      "Epoch 93/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396us/step - accuracy: 0.7089 - loss: 0.7418 - val_accuracy: 0.7080 - val_loss: 0.7365\n",
      "Epoch 94/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - accuracy: 0.7059 - loss: 0.7454 - val_accuracy: 0.7112 - val_loss: 0.7322\n",
      "Epoch 95/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - accuracy: 0.7071 - loss: 0.7460 - val_accuracy: 0.7091 - val_loss: 0.7357\n",
      "Epoch 96/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - accuracy: 0.7059 - loss: 0.7462 - val_accuracy: 0.7088 - val_loss: 0.7413\n",
      "Epoch 97/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - accuracy: 0.7051 - loss: 0.7468 - val_accuracy: 0.7110 - val_loss: 0.7385\n",
      "Epoch 98/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - accuracy: 0.7063 - loss: 0.7486 - val_accuracy: 0.7115 - val_loss: 0.7362\n",
      "Epoch 99/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396us/step - accuracy: 0.7085 - loss: 0.7428 - val_accuracy: 0.7090 - val_loss: 0.7376\n",
      "Epoch 100/100\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - accuracy: 0.7056 - loss: 0.7454 - val_accuracy: 0.7143 - val_loss: 0.7303\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "# build the nn\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
